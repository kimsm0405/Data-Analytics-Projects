import requests
import urllib.parse
from bs4 import BeautifulSoup
import config       # config.py (ì„¤ì •ê°’)
from database import get_db_connection  # database.py (DB ì—°ê²°)
from tmdbv3api import TMDb, Movie

# --- TMDb API ì„¤ì • ---
tmdb = TMDb()
tmdb.api_key = config.TMDB_API_KEY
tmdb.language = 'ko-KR'
movie_api = Movie()

def get_tmdb_info(movie_title):
    """TMDbì—ì„œ ì˜í™” ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        search_results = movie_api.search(movie_title)
        if search_results:
            top_result = search_results[0]
            poster_url = f"{config.TMDB_IMAGE_BASE_URL}{top_result.poster_path}" if top_result.poster_path else None
            rating = top_result.vote_average
            overview = top_result.overview if top_result.overview else None
            tmdb_id = top_result.id
            return poster_url, rating, overview, tmdb_id
    except Exception as e:
        print(f"  âŒ [TMDb ì˜¤ë¥˜] '{movie_title}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
    return None, None, None, None

def check_and_run_etl(target_date):
    """
    DBì— 'target_date'ì˜ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ ,
    ì—†ìœ¼ë©´ ETLì„ ì‹¤í–‰(ë°ì´í„° ê°±ì‹ )í•©ë‹ˆë‹¤.
    (datetime.date ê°ì²´ë¥¼ ì¸ìë¡œ ë°›ìŒ)
    """
    print(f"--- ğŸ’¡ [ìŠ¤ë§ˆíŠ¸ ì„œë²„] '{target_date}' ë‚ ì§œ ë°ì´í„° ì‹ ì„ ë„ ì²´í¬ ---")
    conn = None
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        
        cur.execute("SELECT 1 FROM daily_box_office WHERE target_dt = %s LIMIT 1;", (target_date,))
        
        if cur.fetchone():
            print(f"âœ… [ìŠ¤ë§ˆíŠ¸ ì„œë²„] '{target_date}' ë°ì´í„°ê°€ ì´ë¯¸ DBì— ìˆìŠµë‹ˆë‹¤. ETLì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        print(f"âš ï¸ [ìŠ¤ë§ˆíŠ¸ ì„œë²„] '{target_date}' ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ETLì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        kofic_url = f"http://www.kobis.or.kr/kobisopenapi/webservice/rest/boxoffice/searchDailyBoxOfficeList.json?key={config.KOFIC_API_KEY}&targetDt={target_date.strftime('%Y%m%d')}"
        response = requests.get(kofic_url)
        data = response.json()
        
        if 'boxOfficeResult' not in data or not data['boxOfficeResult']['dailyBoxOfficeList']:
             print(f"â„¹ï¸ [ìŠ¤ë§ˆíŠ¸ ì„œë²„] KOFICì— '{target_date}' ë‚ ì§œì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
             return

        movie_list = data['boxOfficeResult']['dailyBoxOfficeList']

        for movie in movie_list:
            movie_cd = movie['movieCd']
            movie_nm = movie['movieNm']

            cur.execute("SELECT movie_cd FROM movies WHERE movie_cd = %s;", (movie_cd,))
            if cur.fetchone() is None:
                poster, rating, overview, tmdb_id = get_tmdb_info(movie_nm)
                sql_insert_movie = "INSERT INTO movies (movie_cd, movie_nm, poster_url, tmdb_rating, overview, tmdb_id) VALUES (%s, %s, %s, %s, %s, %s) ON CONFLICT (movie_cd) DO NOTHING;"
                cur.execute(sql_insert_movie, (movie_cd, movie_nm, poster, rating, overview, tmdb_id))

            sql_insert_daily = "INSERT INTO daily_box_office (target_dt, rank, movie_cd, audi_cnt, audi_acc) VALUES (%s, %s, %s, %s, %s);"
            cur.execute(sql_insert_daily, (
                target_date, movie['rank'], movie_cd, movie['audiCnt'], movie['audiAcc']
            ))
        
        conn.commit()
        print(f"âœ… [ìŠ¤ë§ˆíŠ¸ ì„œë²„] '{target_date}' ETL ì™„ë£Œ. DBê°€ ê°±ì‹ ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ [ìŠ¤ë§ˆíŠ¸ ì„œë²„] ETL ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        if conn: conn.rollback()
    finally:
        if conn: conn.close()

# --- âœ… [ì‹ ê·œ ì¶”ê°€] êµ¬ê¸€ ì˜í™” ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜ ---
def get_google_movie_news():
    """êµ¬ê¸€ ë‰´ìŠ¤ RSSë¥¼ í†µí•´ ìµœì‹  ì˜í™” ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    keyword = "ì˜í™”"
    encoded_keyword = urllib.parse.quote(keyword)
    # hl=ko: í•œêµ­ì–´, gl=KR: í•œêµ­ ì§€ì—­, ceid=KR:ko
    url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR&ceid=KR:ko"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=3)
        soup = BeautifulSoup(response.content, "xml")
        items = soup.find_all("item")
        
        news_list = []
        # ìµœì‹  5ê°œë§Œ ê°€ì ¸ì˜¤ê¸°
        for item in items[:5]:
            # ë‚ ì§œ í¬ë§· ì •ë¦¬ (ì˜ˆ: Mon, 08 Dec 2025... -> 2025-12-08)
            pub_date = item.pubDate.text if item.pubDate else ""
            
            news_list.append({
                'title': item.title.text,
                'link': item.link.text,
                'date': pub_date,
                'source': item.source.text if item.source else "Google News"
            })
        return news_list
        
    except Exception as e:
        print(f"âŒ [ë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜]: {e}")
        return []